{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/62/aacb236d21fbd08148b1d517d58a9d80ea31bdcd386d26f21f8b23b1eb28/dlib-19.18.0.tar.gz (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 548kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: dlib\n",
      "  Building wheel for dlib (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/thamizharasi/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-qrxn2zsp/dlib/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-qrxn2zsp/dlib/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-b9pyxsw7 --python-tag cp37\n",
      "       cwd: /tmp/pip-install-qrxn2zsp/dlib/\n",
      "  Complete output (53 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  package init file 'dlib/__init__.py' not found (or not a regular file)\n",
      "  running build_ext\n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-qrxn2zsp/dlib/setup.py\", line 120, in get_cmake_version\n",
      "      out = subprocess.check_output(['cmake', '--version'])\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/subprocess.py\", line 395, in check_output\n",
      "      **kwargs).stdout\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/subprocess.py\", line 472, in run\n",
      "      with Popen(*popenargs, **kwargs) as process:\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/subprocess.py\", line 775, in __init__\n",
      "      restore_signals, start_new_session)\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/subprocess.py\", line 1522, in _execute_child\n",
      "      raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "  FileNotFoundError: [Errno 2] No such file or directory: 'cmake': 'cmake'\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-install-qrxn2zsp/dlib/setup.py\", line 261, in <module>\n",
      "      'Topic :: Software Development',\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/site-packages/setuptools/__init__.py\", line 145, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py\", line 192, in run\n",
      "      self.run_command('build')\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/command/build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/tmp/pip-install-qrxn2zsp/dlib/setup.py\", line 129, in run\n",
      "      cmake_version = self.get_cmake_version()\n",
      "    File \"/tmp/pip-install-qrxn2zsp/dlib/setup.py\", line 125, in get_cmake_version\n",
      "      \"\\n*******************************************************************\\n\")\n",
      "  RuntimeError:\n",
      "  *******************************************************************\n",
      "   CMake must be installed to build the following extensions: dlib\n",
      "  *******************************************************************\n",
      "  \n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for dlib\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for dlib\n",
      "Failed to build dlib\n",
      "Installing collected packages: dlib\n",
      "  Running setup.py install for dlib ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /home/thamizharasi/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-qrxn2zsp/dlib/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-qrxn2zsp/dlib/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-63r4dg_y/install-record.txt --single-version-externally-managed --compile\n",
      "         cwd: /tmp/pip-install-qrxn2zsp/dlib/\n",
      "    Complete output (55 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    package init file 'dlib/__init__.py' not found (or not a regular file)\n",
      "    running build_ext\n",
      "    Traceback (most recent call last):\n",
      "      File \"/tmp/pip-install-qrxn2zsp/dlib/setup.py\", line 120, in get_cmake_version\n",
      "        out = subprocess.check_output(['cmake', '--version'])\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/subprocess.py\", line 395, in check_output\n",
      "        **kwargs).stdout\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/subprocess.py\", line 472, in run\n",
      "        with Popen(*popenargs, **kwargs) as process:\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/subprocess.py\", line 775, in __init__\n",
      "        restore_signals, start_new_session)\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/subprocess.py\", line 1522, in _execute_child\n",
      "        raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "    FileNotFoundError: [Errno 2] No such file or directory: 'cmake': 'cmake'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-qrxn2zsp/dlib/setup.py\", line 261, in <module>\n",
      "        'Topic :: Software Development',\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/site-packages/setuptools/__init__.py\", line 145, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/site-packages/setuptools/command/install.py\", line 61, in run\n",
      "        return orig.install.run(self)\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/command/install.py\", line 545, in run\n",
      "        self.run_command('build')\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/command/build.py\", line 135, in run\n",
      "        self.run_command(cmd_name)\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"/home/thamizharasi/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"/tmp/pip-install-qrxn2zsp/dlib/setup.py\", line 129, in run\n",
      "        cmake_version = self.get_cmake_version()\n",
      "      File \"/tmp/pip-install-qrxn2zsp/dlib/setup.py\", line 125, in get_cmake_version\n",
      "        \"\\n*******************************************************************\\n\")\n",
      "    RuntimeError:\n",
      "    *******************************************************************\n",
      "     CMake must be installed to build the following extensions: dlib\n",
      "    *******************************************************************\n",
      "    \n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /home/thamizharasi/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-qrxn2zsp/dlib/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-qrxn2zsp/dlib/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-63r4dg_y/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n",
      "[INFO] serializing 0 encodings...\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "#import dlib\n",
    "#import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import pdb\n",
    "protoPath =  \"./face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"./face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "\n",
    "print(\"[INFO] quantifying faces...\")\n",
    "imagePaths = list(paths.list_images(\"./dataset\"))\n",
    " \n",
    "\n",
    "# initialize the total number of faces processed\n",
    "total = 0\n",
    "\n",
    "#grab the paths to the input images in our dataset, then initialize\n",
    "# out data list (which we'll soon populate)\n",
    "\n",
    "#imagePaths = list(paths.list_images(\"./face-clustering/friendsvideo\"))\n",
    "#imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "data = []\n",
    "\n",
    "#print(imagePaths)\n",
    "\n",
    "\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "embedder = cv2.dnn.readNetFromTorch(\"openface_nn4.small2.v1.t7\")\n",
    "\n",
    "\n",
    "# loop over the image paths\n",
    "total=0\n",
    "knownEmbeddings=[]\n",
    "knownNames=[]\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "\t# extract the person name from the image path\n",
    "\tprint(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "\t\tlen(imagePaths)))\n",
    "\tname = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "\t# load the image, resize it to have a width of 600 pixels (while\n",
    "\t# maintaining the aspect ratio), and then grab the image\n",
    "\t# dimensions\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = imutils.resize(image, width=600)\n",
    "\t(h, w) = image.shape[:2]\n",
    "\n",
    "\t# construct a blob from the image\n",
    "\timageBlob = cv2.dnn.blobFromImage(\n",
    "\t\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "\t# apply OpenCV's deep learning-based face detector to localize\n",
    "\t# faces in the input image\n",
    "\tdetector.setInput(imageBlob)\n",
    "\tdetections = detector.forward()\n",
    "\n",
    "\t# ensure at least one face was found\n",
    "\tif len(detections) > 0:\n",
    "\t\t# we're making the assumption that each image has only ONE\n",
    "\t\t# face, so find the bounding box with the largest probability\n",
    "\t\ti = np.argmax(detections[0, 0, :, 2])\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# ensure that the detection with the largest probability also\n",
    "\t\t# means our minimum probability test (thus helping filter out\n",
    "\t\t# weak detections)\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
    "\t\t\t# the face\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# extract the face ROI and grab the ROI dimensions\n",
    "\t\t\tface = image[startY:endY, startX:endX]\n",
    "\t\t\t(fH, fW) = face.shape[:2]\n",
    "\n",
    "\t\t\t# ensure the face width and height are sufficiently large\n",
    "\t\t\tif fW < 20 or fH < 20:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# construct a blob for the face ROI, then pass the blob\n",
    "\t\t\t# through our face embedding model to obtain the 128-d\n",
    "\t\t\t# quantification of the face\n",
    "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "\t\t\tembedder.setInput(faceBlob)\n",
    "\t\t\tvec = embedder.forward()\n",
    "\n",
    "\t\t\t# add the name of the person + corresponding face\n",
    "\t\t\t# embedding to their respective lists\n",
    "\t\t\tknownNames.append(name)\n",
    "\t\t\tknownEmbeddings.append(vec.flatten())\n",
    "\ttotal += 1\n",
    "# dump the facial encodings data to disk\n",
    "\n",
    "# # dump the facial embeddings + names to disk\n",
    "# print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "\n",
    "# dump the facial encodings + names to disk\n",
    "print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "f = open(\"./encodingsclass\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face embeddings...\n",
      "[INFO] encoding labels...\n",
      "[INFO] training model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4abb362409b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mrecognizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# write the actual face recognition model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recognize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    144\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    145\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import argparse\n",
    "import pickle\n",
    "import imutils\n",
    "\n",
    "# load the face embeddings\n",
    "print(\"[INFO] loading face embeddings...\")\n",
    "data = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    " \n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])\n",
    "# train the model used to accept the 128-d embeddings of the face and\n",
    "# then produce the actual face recognition\n",
    "print(\"[INFO] training model...\")\n",
    "recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)\n",
    "# write the actual face recognition model to disk\n",
    "f = open(\"recognize\", \"wb\")\n",
    "f.write(pickle.dumps(recognizer))\n",
    "f.close()\n",
    " \n",
    "# write the label encoder to disk\n",
    "f = open(\"lee\", \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ebcd158a5b8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# import the necessary packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "# FACE classification using SVM classifier\n",
    "# python recognize_faces_image.py --encodings encodings.pickle --image examples/example_01.png \n",
    "\n",
    "# import the necessary packages\n",
    "import face_recognition\n",
    "import argparse\n",
    "import pickle\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "dir=\"./images/\"\n",
    "protoPath =  \"./face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"./face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "embedder = cv2.dnn.readNetFromTorch(\"openface_nn4.small2.v1.t7\")\n",
    "\n",
    "\n",
    "# load the actual face recognition model along with the label encoder\n",
    "recognizer = pickle.loads(open(\"recognize\", \"rb\").read())\n",
    "le = pickle.loads(open(\"lee\", \"rb\").read())\n",
    "\n",
    "\n",
    "\n",
    "# # load the known faces and embeddings\n",
    "# print(\"[INFO] loading encodings...\")\n",
    "# data = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    "\n",
    "# load the input image and convert it from BGR to RGB\n",
    "imgname=input(\"Enter person name\")\n",
    "filename= dir+imgname+\".jpg\"\n",
    "if os.path.isfile(filename)!=True:\n",
    "    print(\"Person name doesn't exists\")\n",
    "else:\n",
    "    image = cv2.imread(filename)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections\n",
    "        if confidence > 0.5:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the\n",
    "            # face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # extract the face ROI\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n",
    "                (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "\n",
    "            # perform classification to recognize the face\n",
    "            preds = recognizer.predict_proba(vec)[0]\n",
    "            j = np.argmax(preds)\n",
    "            proba = preds[j]\n",
    "            name = le.classes_[j]\n",
    "\n",
    "            # draw the bounding box of the face along with the associated\n",
    "            # probability\n",
    "            text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            cv2.putText(image, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "    # show the output image\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE DETECTION RECOGNITION USING DLIB WITH COMPARISON IN VIDEO FILE\n",
    "\n",
    "# import the necessary packages\n",
    "import face_recognition\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "from imutils.video import FPS\n",
    "\n",
    "protoPath =  \"./face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"./face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "embedder = cv2.dnn.readNetFromTorch(\"openface_nn4.small2.v1.t7\")\n",
    "\n",
    "# load the face embeddings\n",
    "print(\"[INFO] loading face embeddings...\")\n",
    "data = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    " \n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])\n",
    "# train the model used to accept the 128-d embeddings of the face and\n",
    "# then produce the actual face recognition\n",
    "print(\"[INFO] training model...\")\n",
    "recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)\n",
    "# write the actual face recognition model to disk\n",
    "f = open(\"recognize\", \"wb\")\n",
    "f.write(pickle.dumps(recognizer))\n",
    "f.close()\n",
    " \n",
    "# write the label encoder to disk\n",
    "f = open(\"lee\", \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()\n",
    "\n",
    "\n",
    "# load the actual face recognition model along with the label encoder\n",
    "recognizer = pickle.loads(open(\"recognize\", \"rb\").read())\n",
    "le = pickle.loads(open(\"lee\", \"rb\").read())\n",
    "\n",
    "# # load the known faces and embeddings\n",
    "# print(\"[INFO] loading encodings...\")\n",
    "# data = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    "\n",
    "# initialize the pointer to the video file and the video writer\n",
    "print(\"[INFO] processing video...\")\n",
    "stream = cv2.VideoCapture(\"adrian.mp4\")\n",
    "fps = FPS().start()\n",
    "writer = None\n",
    "display=1\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "\t# grab the next frame\n",
    "\t(grabbed, frame) = stream.read()\n",
    "\n",
    "\t# if the frame was not grabbed, then we have reached the\n",
    "\t# end of the stream\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "    \n",
    "\timage = imutils.resize(frame, width=600)\n",
    "\t(h, w) = image.shape[:2]\n",
    "\tr = frame.shape[1] / float(image.shape[1])\n",
    "    \n",
    "    # construct a blob from the image\n",
    "\timageBlob = cv2.dnn.blobFromImage(\n",
    "\t\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "\tdetector.setInput(imageBlob)\n",
    "\tdetections = detector.forward()\n",
    "    # loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with the\n",
    "\t\t# prediction\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# filter out weak detections\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for the\n",
    "\t\t\t## face\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t## extract the face ROI\n",
    "\t\t\tface = image[startY:endY, startX:endX]\n",
    "\t\t\t(fH, fW) = face.shape[:2]\n",
    "\n",
    "            # ensure the face width and height are sufficiently large\n",
    "\t\t\tif fW < 20 or fH < 20:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\n",
    "\t\t\t# construct a blob for the face ROI, then pass the blob\n",
    "\t\t\t# through our face embedding model to obtain the 128-d\n",
    "\t\t\t# quantification of the face\n",
    "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n",
    "\t\t\t\t(0, 0, 0), swapRB=True, crop=False)\n",
    "\t\t\tembedder.setInput(faceBlob)\n",
    "\t\t\tvec = embedder.forward()\n",
    "\n",
    "            # perform classification to recognize the face\n",
    "\t\t\tpreds = recognizer.predict_proba(vec)[0]\n",
    "\t\t\tj = np.argmax(preds)\n",
    "\t\t\tproba = preds[j]\n",
    "\t\t\tname = le.classes_[j]\n",
    "\n",
    "           # draw the bounding box of the face along with the associated\n",
    "            # probability\n",
    "\t\t\ttext = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "\t\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "\t\t\t\t(0, 0, 255), 2)\n",
    "\t\t\tcv2.putText(frame, text, (startX, y),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "    # show the output image\n",
    "# \tcv2.imshow(\"Image\", image)\n",
    "# \tcv2.waitKey(0)\n",
    " \n",
    "\tif display > 0:\n",
    "\t\tcv2.imshow(\"Frame\", frame)\n",
    "\t\tfps.update()\n",
    "\t\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t\t# if the `q` key was pressed, break from the loop\n",
    "\t\tif key == ord(\"q\"):\n",
    "\t\t\tbreak\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    " \n",
    "# do a bit of cleanup\n",
    "stream.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# check to see if the video writer point needs to be released\n",
    "if writer is not None:\n",
    "\twriter.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Face matching in video\n",
    "# python recognize_faces_video.py --encodings encodings.pickle\n",
    "# python recognize_faces_video.py --encodings encodings.pickle --output output/jurassic_park_trailer_output.avi --display 0\n",
    "\n",
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils import paths\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import face_recognition\n",
    "import dlib\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "protoPath =  \"./face_detection_model/deploy.prototxt\"\n",
    "modelPath = \"./face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "embedder = cv2.dnn.readNetFromTorch(\"openface_nn4.small2.v1.t7\")\n",
    "\n",
    "# load the face embeddings\n",
    "print(\"[INFO] loading face embeddings...\")\n",
    "data = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    " \n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])\n",
    "# train the model used to accept the 128-d embeddings of the face and\n",
    "# then produce the actual face recognition\n",
    "print(\"[INFO] training model...\")\n",
    "recognizer = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)\n",
    "# write the actual face recognition model to disk\n",
    "f = open(\"recognize\", \"wb\")\n",
    "f.write(pickle.dumps(recognizer))\n",
    "f.close()\n",
    " \n",
    "# write the label encoder to disk\n",
    "f = open(\"lee\", \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()\n",
    "\n",
    "\n",
    "# load the actual face recognition model along with the label encoder\n",
    "recognizer = pickle.loads(open(\"recognize\", \"rb\").read())\n",
    "le = pickle.loads(open(\"lee\", \"rb\").read())\n",
    "\n",
    "# # load the known faces and embeddings\n",
    "# print(\"[INFO] loading encodings...\")\n",
    "# data = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    "\n",
    "\n",
    "# # load the known faces and embeddings\n",
    "# print(\"[INFO] loading encodings...\")\n",
    "# data = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    "\n",
    "# initialize the video stream and pointer to output video file, then\n",
    "# allow the camera sensor to warm up\n",
    "print(\"[INFO] starting video stream...\")\n",
    "_videodir = \"./dataset\"\n",
    "#vs = cv2.VideoCapture(0)\n",
    "vs = VideoStream(src=0).start()\n",
    "writer = None\n",
    "time.sleep(2.0)\n",
    "display=1\n",
    "count=0\n",
    "saveimage='n'\n",
    "\n",
    "# unknownfolder=\"./dataset/unknown1\"\n",
    "# if os.path.exists(unknownfolder)==True:\n",
    "# #Remove images in temporary unknown1 folder                 \n",
    "# \tfilelist = [ f for f in os.listdir(unknownfolder) if f.endswith(\".jpg\") ]\n",
    "# \tfor f in filelist:\n",
    "# \t\tos.remove(os.path.join(_unknownfolder, f))    \n",
    "# # loop over frames from the video file stream\n",
    "while True:\n",
    "\t# grab the frame from the threaded video stream\n",
    "#\t(grabbed, frame) = vs.read()\n",
    "\tframe = vs.read()\n",
    "\t# if the frame was not grabbed, then we have reached the\n",
    "\t# end of the stream\n",
    "# \tif not grabbed:\n",
    "# \t\tbreak\n",
    "    \n",
    "\timage = imutils.resize(frame, width=600)\n",
    "\t(h, w) = image.shape[:2]\n",
    "\tr = frame.shape[1] / float(image.shape[1])\n",
    "    \n",
    "    # construct a blob from the image\n",
    "\timageBlob = cv2.dnn.blobFromImage(\n",
    "\t\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "\tdetector.setInput(imageBlob)\n",
    "\tdetections = detector.forward()\n",
    "    # loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with the\n",
    "\t\t# prediction\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# filter out weak detections\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for the\n",
    "\t\t\t## face\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t## extract the face ROI\n",
    "\t\t\tface = image[startY:endY, startX:endX]\n",
    "\t\t\t(fH, fW) = face.shape[:2]\n",
    "\n",
    "            # ensure the face width and height are sufficiently large\n",
    "\t\t\tif fW < 20 or fH < 20:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\n",
    "\t\t\t# construct a blob for the face ROI, then pass the blob\n",
    "\t\t\t# through our face embedding model to obtain the 128-d\n",
    "\t\t\t# quantification of the face\n",
    "\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96),\n",
    "\t\t\t\t(0, 0, 0), swapRB=True, crop=False)\n",
    "\t\t\tembedder.setInput(faceBlob)\n",
    "\t\t\tvec = embedder.forward()\n",
    "\n",
    "            # perform classification to recognize the face\n",
    "\t\t\tpreds = recognizer.predict_proba(vec)[0]\n",
    "\t\t\tj = np.argmax(preds)\n",
    "\t\t\tproba = preds[j]\n",
    "\t\t\tname = le.classes_[j]\n",
    "\n",
    "           # draw the bounding box of the face along with the associated\n",
    "            # probability\n",
    "\t\t\ttext = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "\t\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "\t\t\t\t(0, 0, 255), 2)\n",
    "\t\t\tcv2.putText(frame, text, (startX, y),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "            \n",
    "            \n",
    "            \n",
    "# \t# convert the input frame from BGR to RGB then resize it to have\n",
    "# \t# a width of 750px (to speedup processing)\n",
    "# \trgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "# \trgb = imutils.resize(frame, width=750)\n",
    "# \tr = frame.shape[1] / float(rgb.shape[1])\n",
    "\n",
    "# \t# detect the (x, y)-coordinates of the bounding boxes\n",
    "# \t# corresponding to each face in the input frame, then compute\n",
    "# \t# the facial embeddings for each face\n",
    "# \tboxes = face_recognition.face_locations(rgb, model=\"hog\")\n",
    "# \tencodings = face_recognition.face_encodings(rgb, boxes)\n",
    "# \tnames = []\n",
    "\n",
    "# \t# loop over the facial embeddings\n",
    "# \tfor encoding in encodings:\n",
    "# \t\t# attempt to match each face in the input image to our known\n",
    "# \t\t# encodings\n",
    "# \t\tmatches = face_recognition.compare_faces(data[\"encodings\"],\n",
    "# \t\t\tencoding)\n",
    "# \t\tname = \"Unknown\"\n",
    "\n",
    "# \t\t# check to see if we have found a match\n",
    "# \t\tif True in matches:\n",
    "# \t\t\t# find the indexes of all matched faces then initialize a\n",
    "# \t\t\t# dictionary to count the total number of times each face\n",
    "# \t\t\t# was matched\n",
    "# \t\t\tmatchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "# \t\t\tcounts = {}\n",
    "\n",
    "# \t\t\t# loop over the matched indexes and maintain a count for\n",
    "# \t\t\t# each recognized face face\n",
    "# \t\t\tfor i in matchedIdxs:\n",
    "# \t\t\t\tname = data[\"names\"][i]\n",
    "# \t\t\t\tcounts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "# \t\t\t# determine the recognized face with the largest number\n",
    "# \t\t\t# of votes (note: in the event of an unlikely tie Python\n",
    "# \t\t\t# will select first entry in the dictionary)\n",
    "# \t\t\tname = max(counts, key=counts.get)\n",
    "\t\t\n",
    "# \t\t# update the list of names\n",
    "# \t\tnames.append(name)\n",
    "\n",
    "# \t# loop over the recognized faces\n",
    "# \tfor ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "# \t\t# rescale the face coordinates\n",
    "# \t\ttop = int(top * r)\n",
    "# \t\tright = int(right * r)\n",
    "# \t\tbottom = int(bottom * r)\n",
    "# \t\tleft = int(left * r)\n",
    "\n",
    "# \t\t# draw the predicted face name on the image\n",
    "# \t\tcv2.rectangle(frame, (left, top), (right, bottom),\n",
    "# \t\t\t(0, 255, 0), 2)\n",
    "# \t\ty = top - 15 if top - 15 > 15 else top + 15\n",
    "# \t\tcv2.putText(frame, name , (left, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "# \t\t\t0.75, (0, 255, 0), 2)\n",
    "\t\t\tcv2.imwrite(\"./dataset/unknown1/frame%d.jpg\" %count,frame)\n",
    "\t\t\tcount+=1\n",
    "# \t# if the video writer is None *AND* we are supposed to write\n",
    "# \t# the output video to disk initialize the writer\n",
    "# \tif writer is None and output is not None:\n",
    "# \t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "# \t\twriter = cv2.VideoWriter(output, fourcc, 20,\n",
    "# \t\t\t(frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "# \t# if the writer is not None, write the frame with recognized\n",
    "# \t# faces t odisk\n",
    "# \tif writer is not None:\n",
    "#  \t\twriter.write(frame)\n",
    "\n",
    "# # \t# check to see if we are supposed to display the output frame to\n",
    "# # \t# the screen\n",
    "\tif display > 0:\n",
    "\t\tcv2.imshow(\"Frame\", frame)\n",
    "\t\t\n",
    "\t\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t\t# if the `q` key was pressed, break from the loop\n",
    "\t\tif key == ord(\"q\"):\n",
    "\t\t\tbreak\n",
    "\n",
    "# do a bit of cleanup\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n",
    "\n",
    "# check to see if the video writer point needs to be released\n",
    "if writer is not None:\n",
    "\twriter.release()\n",
    "    \n",
    "\n",
    "#store new person\n",
    "\n",
    "\n",
    "# from imutils.video import VideoStream\n",
    "# import face_recognition\n",
    "# import argparse\n",
    "# import imutils\n",
    "# import dlib\n",
    "# import pickle\n",
    "# import time\n",
    "# import cv2\n",
    "\n",
    "# AUTHENTICATE PERSON\n",
    "\n",
    "_videodir= \"./dataset/\"\n",
    "_videodir1 = \"./dataset/unknown1/\"\n",
    "if name=='unknown':\n",
    "\tsaveimage=input(\"Do you want to Authenticate this person ( Press y/n)?\")\n",
    "\tif saveimage=='y':\n",
    "\t\tpersonname=input(\"Enter person name \")                \n",
    "            \n",
    "# create dynamic name, like \"D:\\Current Download\\Attachment82673\"\n",
    "\t\t_videodir2= os.path.join(_videodir, personname)\n",
    "\n",
    "        # create 'dynamic' dir, if it does not exist\n",
    "\t\tif not os.path.exists(_videodir2):\n",
    "\t\t\tos.makedirs(_videodir2)\n",
    "\t\tlisting = os.listdir(_videodir1)    \n",
    "\t\tfor file in listing:\n",
    "\t\t\timg = cv2.imread(_videodir1+file)\n",
    "\t\t\toutfile= _videodir2+\"/\"+file\n",
    "\t\t\tcv2.imwrite(outfile,img)\n",
    "\n",
    "#Remove images in temporary unknown1 folder\n",
    "                            \n",
    "\n",
    "\t\tfilelist = [ f for f in os.listdir(_videodir1) if f.endswith(\".jpg\") ]\n",
    "\t\tfor f in filelist:\n",
    "\t\t\tos.remove(os.path.join(_videodir1, f))\n",
    "\n",
    "\t\t#_videodir2= \"./dataset/tamilarasi\"\n",
    "\n",
    "#store face encodings of new person\n",
    "\n",
    "\n",
    "# loop over the image paths\n",
    "\t\ttotal1=0\n",
    "\t\tknownEmbeddings1=[]\n",
    "\t\tknownNames1=[]\n",
    "\t\timagePaths = list(paths.list_images(_videodir2))\n",
    "\n",
    "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
    "\t\t\t# extract the person name from the image path\n",
    "\t\t\tprint(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "\t\t\t\tlen(imagePaths)))\n",
    "\t\t\tname = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "\t\t\t# load the image, resize it to have a width of 600 pixels (while\n",
    "\t\t\t# maintaining the aspect ratio), and then grab the image\n",
    "\t\t\t# dimensions\n",
    "            \n",
    "\t\t\timage = cv2.imread(imagePath)\n",
    "\t\t\timage = imutils.resize(image, width=600)\n",
    "\t\t\t(h, w) = image.shape[:2]\n",
    "\n",
    "\t\t\t# construct a blob from the image\n",
    "\t\t\timageBlob = cv2.dnn.blobFromImage(\n",
    "\t\t\t\tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "\t\t\t\t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "\t\t\t# apply OpenCV's deep learning-based face detector to localize\n",
    "\t\t\t# faces in the input image\n",
    "\t\t\tdetector.setInput(imageBlob)\n",
    "\t\t\tdetections = detector.forward()\n",
    "\n",
    "\t\t\t# ensure at least one face was found\n",
    "\t\t\tif len(detections) > 0:\n",
    "\t\t\t# we're making the assumption that each image has only ONE\n",
    "\t\t\t# face, so find the bounding box with the largest probability\n",
    "\t\t\t\ti = np.argmax(detections[0, 0, :, 2])\n",
    "\t\t\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t\t# ensure that the detection with the largest probability also\n",
    "\t\t\t# means our minimum probability test (thus helping filter out\n",
    "\t\t\t# weak detections)\n",
    "\t\t\t\tif confidence > 0.5:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
    "\t\t\t# the face\n",
    "\t\t\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# extract the face ROI and grab the ROI dimensions\n",
    "\t\t\t\t\tface = image[startY:endY, startX:endX]\n",
    "\t\t\t\t\t(fH, fW) = face.shape[:2]\n",
    "\n",
    "\t\t\t\t# ensure the face width and height are sufficiently large\n",
    "\t\t\t\t\tif fW < 20 or fH < 20:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# construct a blob for the face ROI, then pass the blob\n",
    "\t\t\t# through our face embedding model to obtain the 128-d\n",
    "\t\t\t# quantification of the face\n",
    "\t\t\t\t\tfaceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "\t\t\t\t\t\t(96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "\t\t\t\t\tembedder.setInput(faceBlob)\n",
    "\t\t\t\t\tvec = embedder.forward()\n",
    "\n",
    "\t\t\t\t\t# add the name of the person + corresponding face\n",
    "\t\t\t\t\t# embedding to their respective lists\n",
    "\t\t\t\t\tknownNames1.append(name)\n",
    "\t\t\t\t\tknownEmbeddings1.append(vec.flatten())\n",
    "                \n",
    "# #\t\t\timage = cv2.imread(imagePath)\n",
    "# \t\t\trgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# \t\t\t # detect the (x, y)-coordinates of the bounding boxes\n",
    "# \t\t\t# corresponding to each face in the input image\n",
    "# \t\t\tboxes = face_recognition.face_locations(rgb,model=\"hog\")\n",
    "# \t\t\t# compute the facial embedding for the face\n",
    "# \t\t\tencodings = face_recognition.face_encodings(rgb, boxes)\n",
    " \n",
    "# \t\t\tfor encoding in encodings:\n",
    "# \t\t\t\t# add each encoding + name to our set of known names and\n",
    "# \t\t\t\t# encodings\n",
    "# \t\t\t\tknownEncodings1.append(encoding)\n",
    "# \t\t\t\tknownNames1.append(name)\n",
    "\t\t\ttotal1 += 1\n",
    "    # build a dictionary of the image path, bounding box location,\n",
    "\t# and facial encodings for the current image\n",
    "\n",
    "\t\n",
    "# dump the facial encodings data to disk\n",
    "\n",
    "\n",
    "# dump the facial encodings + names of new person to disk\n",
    "\t\tprint(\"[INFO] loading new encodings...\")\n",
    "\t\tdata = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    "\n",
    "\t\tprint(\"[INFO] serializing {} encodings...\".format(total1))\n",
    "\t\tdata1 = {\"embeddings\": knownEmbeddings1, \"names\": knownNames1}\n",
    "\t\tf = open(\"./encodingsclass\", \"wb\")\n",
    "\t\tdata.update(data1)\n",
    "\t\tf.write(pickle.dumps(data))\n",
    "\t\tf.close()\n",
    "elif (saveimage!='y'):\n",
    "\tprint(\"Not Authenticated person. Raise notification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.loads(open(\"./encodingsclass\", \"rb\").read())\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
